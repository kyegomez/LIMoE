[![Multi-Modality](agorabanner.png)](https://discord.gg/qUtxnK2NMf)

# LiMoE
Implementation of the "the first large-scale multimodal mixture of experts models." from the paper: "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts". [CLICK HERE FOR THE PAPER LINK:](https://arxiv.org/abs/2206.02770)


# install
`pip install limoe`



# License
MIT
